{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ce880597",
      "metadata": {
        "id": "ce880597"
      },
      "source": [
        "# LLMSQL2 Colab GPU Training\n",
        "\n",
        "This notebook trains the required models and saves checkpoints to Google Drive.\n",
        "\n",
        "This version is set up to train the **remaining databases** (e.g., advising + restaurants) since **atis** and **geography** are already trained.\n",
        "\n",
        "**Important:** Runtime → Change runtime type → **GPU**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30cec32c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30cec32c",
        "outputId": "aeb369d0-562f-4345-d630-b12ebe89d878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Feb  4 08:53:44 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Verify GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b043d00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b043d00",
        "outputId": "51f86a36-ab29-4c34-e185-a4d4acf681ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (for saving checkpoints)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YC0WonNxgZw6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC0WonNxgZw6",
        "outputId": "0c2246d5-6071-4e07-ec0b-d672f90710a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "GitHub username: Sskarm\n",
            "GitHub Personal Access Token (PAT): ··········\n",
            "/content/Information-Systems/dthivaios/LLMSQL2\n",
            "analyze_complexity.py\tDockerfile.gpu\t  src\n",
            "data\t\t\tdocs\t\t  test_databases.py\n",
            "docker\t\t\tnotebooks\t  test_db_connections.py\n",
            "docker-compose.gpu.yml\tREADME.md\t  test_model_db_integration.py\n",
            "docker-compose.yml\trequirements.txt  train_all_models.py\n",
            "Dockerfile\t\trun.bat\t\t  train_tinyllama.py\n"
          ]
        }
      ],
      "source": [
        "# --- Google Colab: clone a PRIVATE GitHub repo (PAT method) ---\n",
        "\n",
        "%cd /content\n",
        "\n",
        "import getpass, os\n",
        "\n",
        "GH_USER = input(\"GitHub username: \").strip()\n",
        "GH_TOKEN = getpass.getpass(\"GitHub Personal Access Token (PAT): \").strip()\n",
        "\n",
        "# Clone (token is NOT printed because we don't echo the command output with the token)\n",
        "repo_url = f\"https://{GH_USER}:{GH_TOKEN}@github.com/Sskarm/Information-Systems.git\"\n",
        "\n",
        "# Optional: avoid leaving the token in Colab output by doing it via a small shell script\n",
        "import subprocess\n",
        "subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
        "\n",
        "# Go to your project folder (adjust if your path differs)\n",
        "%cd /content/Information-Systems/dthivaios/LLMSQL2\n",
        "\n",
        "# Quick check\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7406d64f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7406d64f",
        "outputId": "c7e539f4-498f-4ce9-e5c1-0903fc8a4a43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Information-Systems/dthivaios/LLMSQL2\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Information-Systems/dthivaios/LLMSQL2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb8bc515",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb8bc515",
        "outputId": "ebd7262e-22a5-4207-b9ee-68349207f520"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/88.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.0/118.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m140.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==6.17.1, but you have ipykernel 7.1.0 which is incompatible.\n",
            "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.8.0 which is incompatible.\n",
            "jupyter-kernel-gateway 2.5.2 requires notebook<7.0,>=5.7.6, but you have notebook 7.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip -q install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73dda8e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure compatible Hugging Face versions for TrainingArguments\n",
        "!pip -q install -U \"transformers>=4.41.0\" \"accelerate>=0.30.0\" \"peft>=0.10.0\"\n",
        "import transformers, accelerate, peft\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"accelerate:\", accelerate.__version__)\n",
        "print(\"peft:\", peft.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "086d4443",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "086d4443",
        "outputId": "e2adcba3-2afc-4450-8a25-6500592af5d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026-02-04 08:54:47] Cloning text2sql-data repository...\n",
            "Cloning into 'text2sql-data'...\n",
            "Updating files:  36% (375/1041)\rUpdating files:  37% (386/1041)\rUpdating files:  38% (396/1041)\rUpdating files:  39% (406/1041)\rUpdating files:  40% (417/1041)\rUpdating files:  41% (427/1041)\rUpdating files:  42% (438/1041)\rUpdating files:  43% (448/1041)\rUpdating files:  44% (459/1041)\rUpdating files:  45% (469/1041)\rUpdating files:  46% (479/1041)\rUpdating files:  47% (490/1041)\rUpdating files:  48% (500/1041)\rUpdating files:  49% (511/1041)\rUpdating files:  50% (521/1041)\rUpdating files:  51% (531/1041)\rUpdating files:  52% (542/1041)\rUpdating files:  53% (552/1041)\rUpdating files:  54% (563/1041)\rUpdating files:  55% (573/1041)\rUpdating files:  56% (583/1041)\rUpdating files:  57% (594/1041)\rUpdating files:  58% (604/1041)\rUpdating files:  59% (615/1041)\rUpdating files:  60% (625/1041)\rUpdating files:  61% (636/1041)\rUpdating files:  62% (646/1041)\rUpdating files:  63% (656/1041)\rUpdating files:  64% (667/1041)\rUpdating files:  65% (677/1041)\rUpdating files:  66% (688/1041)\rUpdating files:  67% (698/1041)\rUpdating files:  68% (708/1041)\rUpdating files:  69% (719/1041)\rUpdating files:  70% (729/1041)\rUpdating files:  71% (740/1041)\rUpdating files:  72% (750/1041)\rUpdating files:  73% (760/1041)\rUpdating files:  74% (771/1041)\rUpdating files:  75% (781/1041)\rUpdating files:  76% (792/1041)\rUpdating files:  77% (802/1041)\rUpdating files:  78% (812/1041)\rUpdating files:  79% (823/1041)\rUpdating files:  80% (833/1041)\rUpdating files:  81% (844/1041)\rUpdating files:  82% (854/1041)\rUpdating files:  83% (865/1041)\rUpdating files:  84% (875/1041)\rUpdating files:  85% (885/1041)\rUpdating files:  86% (896/1041)\rUpdating files:  87% (906/1041)\rUpdating files:  88% (917/1041)\rUpdating files:  89% (927/1041)\rUpdating files:  90% (937/1041)\rUpdating files:  91% (948/1041)\rUpdating files:  92% (958/1041)\rUpdating files:  93% (969/1041)\rUpdating files:  94% (979/1041)\rUpdating files:  95% (989/1041)\rUpdating files:  96% (1000/1041)\rUpdating files:  97% (1010/1041)\rUpdating files:  98% (1021/1041)\rUpdating files:  99% (1031/1041)\rUpdating files: 100% (1041/1041)\rUpdating files: 100% (1041/1041), done.\n",
            "[2026-02-04 08:54:52] ✓ Datasets downloaded\n"
          ]
        }
      ],
      "source": [
        "# Download the text2sql datasets (text2sql-data repo) + log progress\n",
        "%%bash\n",
        "set -e\n",
        "\n",
        "# ---- Paths (adjust if your project lives elsewhere) ----\n",
        "PROJECT_DIR=\"/content/Information-Systems/dthivaios/LLMSQL2\"\n",
        "DATA_DIR=\"$PROJECT_DIR/data\"\n",
        "LOG_FILE=\"$PROJECT_DIR/setup.log\"\n",
        "SCRIPT_DIR=\"$PROJECT_DIR\"\n",
        "\n",
        "log () { echo \"[$(date '+%F %T')] $1\" | tee -a \"$LOG_FILE\"; }\n",
        "\n",
        "log \"Cloning text2sql-data repository...\"\n",
        "mkdir -p \"$DATA_DIR\"\n",
        "cd \"$DATA_DIR\"\n",
        "\n",
        "# If the repo already exists, skip cloning\n",
        "if [ -d \"text2sql-data/.git\" ]; then\n",
        "  log \"text2sql-data already present — skipping clone.\"\n",
        "else\n",
        "  git clone https://github.com/jkkummerfeld/text2sql-data.git 2>&1 | tee -a \"$LOG_FILE\"\n",
        "fi\n",
        "\n",
        "cd \"$SCRIPT_DIR\"\n",
        "log \"✓ Datasets downloaded\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaacc505",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaacc505",
        "outputId": "8ee51481-eb80-4f3a-dbb9-6f2d196046b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results will be saved to: /content/drive/MyDrive/LLMSQL2/results\n"
          ]
        }
      ],
      "source": [
        "# Set output paths in Drive\n",
        "GDRIVE_OUT = '/content/drive/MyDrive/LLMSQL2/results'\n",
        "!mkdir -p {GDRIVE_OUT}\n",
        "print('Results will be saved to:', GDRIVE_OUT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caa3aa30",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select which databases to train/evaluate\n",
        "DATASETS_TO_TRAIN = [\"advising\", \"restaurants\"]  # atis + geography already trained\n",
        "EPOCHS = {\"gpt2\": 5, \"tinyllama\": 3}\n",
        "BATCH_SIZE_GPT2 = 2\n",
        "\n",
        "print(\"Datasets to train:\", DATASETS_TO_TRAIN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fd6a8c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fd6a8c1",
        "outputId": "9fdee924-50da-4da7-d0c6-e099c107a2a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-04 09:04:41,003 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n",
            "2026-02-04 09:04:41,982 - datasets - INFO - TensorFlow version 2.19.0 available.\n",
            "2026-02-04 09:04:41,983 - datasets - INFO - JAX version 0.7.2 available.\n",
            "2026-02-04 09:04:42,233 - src.utils - INFO - Starting training with config: TrainingConfig(model_name='n22t7a/text2sql-tuned-gpt2', output_dir='/content/drive/MyDrive/LLMSQL2/results/gpt2-geography', num_epochs=5, batch_size=2, learning_rate=5e-05, max_length=256, warmup_steps=100, save_steps=500, logging_steps=50)\n",
            "2026-02-04 09:04:42,234 - src.utils - INFO - Loading model: n22t7a/text2sql-tuned-gpt2\n",
            "2026-02-04 09:04:42,399 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/n22t7a/text2sql-tuned-gpt2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-04 09:04:42,479 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/n22t7a/text2sql-tuned-gpt2/79eba040124809c12b5154257d5032f083589607/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-04 09:04:42,573 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/n22t7a/text2sql-tuned-gpt2/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-04 09:04:42,654 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/n22t7a/text2sql-tuned-gpt2/79eba040124809c12b5154257d5032f083589607/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-04 09:04:42,745 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/n22t7a/text2sql-tuned-gpt2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-04 09:04:42,838 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/n22t7a/text2sql-tuned-gpt2/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
            "2026-02-04 09:04:43,374 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/n22t7a/text2sql-tuned-gpt2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-04 09:04:43,391 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/n22t7a/text2sql-tuned-gpt2/79eba040124809c12b5154257d5032f083589607/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-04 09:04:43,495 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/n22t7a/text2sql-tuned-gpt2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-04 09:04:43,512 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/n22t7a/text2sql-tuned-gpt2/79eba040124809c12b5154257d5032f083589607/config.json \"HTTP/1.1 200 OK\"\n",
            "Loading weights: 100% 76/76 [00:00<00:00, 3194.51it/s, Materializing param=transformer.wte.weight]\n",
            "2026-02-04 09:04:43,675 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/n22t7a/text2sql-tuned-gpt2/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "2026-02-04 09:04:43,676 - huggingface_hub.utils._http - WARNING - Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "2026-02-04 09:04:43,757 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/n22t7a/text2sql-tuned-gpt2/79eba040124809c12b5154257d5032f083589607/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-04 09:04:43,760 - src.utils - INFO - Loaded 246 examples from /content/Information-Systems/dthivaios/LLMSQL2/data/text2sql-data/data/geography.json\n",
            "2026-02-04 09:04:43,761 - src.utils - INFO - Prepared 877 training examples\n",
            "Map: 100% 877/877 [00:00<00:00, 1766.50 examples/s]\n",
            "2026-02-04 09:04:45,402 - src.utils - INFO - Starting training...\n",
            "  0% 0/2195 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
            "{'loss': '3.917', 'grad_norm': '4.659', 'learning_rate': '2.45e-05', 'epoch': '0.1139'}\n",
            "{'loss': '0.7821', 'grad_norm': '3.655', 'learning_rate': '4.95e-05', 'epoch': '0.2278'}\n",
            "{'loss': '0.2187', 'grad_norm': '1.609', 'learning_rate': '4.883e-05', 'epoch': '0.3417'}\n",
            "{'loss': '0.1821', 'grad_norm': '2.124', 'learning_rate': '4.764e-05', 'epoch': '0.4556'}\n",
            "{'loss': '0.1372', 'grad_norm': '1.578', 'learning_rate': '4.644e-05', 'epoch': '0.5695'}\n",
            "{'loss': '0.1665', 'grad_norm': '2.65', 'learning_rate': '4.525e-05', 'epoch': '0.6834'}\n",
            "{'loss': '0.1491', 'grad_norm': '1.881', 'learning_rate': '4.406e-05', 'epoch': '0.7973'}\n",
            "{'loss': '0.1342', 'grad_norm': '1.166', 'learning_rate': '4.286e-05', 'epoch': '0.9112'}\n",
            "{'loss': '0.1308', 'grad_norm': '2.287', 'learning_rate': '4.167e-05', 'epoch': '1.025'}\n",
            "{'loss': '0.1081', 'grad_norm': '0.9588', 'learning_rate': '4.048e-05', 'epoch': '1.139'}\n",
            " 23% 500/2195 [00:33<01:54, 14.81it/s]\n",
            "Writing model shards:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Writing model shards: 100% 1/1 [00:04<00:00,  4.17s/it]\n",
            "{'loss': '0.1183', 'grad_norm': '1.684', 'learning_rate': '3.928e-05', 'epoch': '1.253'}\n",
            "{'loss': '0.1058', 'grad_norm': '0.801', 'learning_rate': '3.809e-05', 'epoch': '1.367'}\n",
            "{'loss': '0.09802', 'grad_norm': '1.185', 'learning_rate': '3.69e-05', 'epoch': '1.481'}\n",
            "{'loss': '0.1026', 'grad_norm': '1.092', 'learning_rate': '3.57e-05', 'epoch': '1.595'}\n",
            "{'loss': '0.09927', 'grad_norm': '1.149', 'learning_rate': '3.451e-05', 'epoch': '1.708'}\n",
            "{'loss': '0.09065', 'grad_norm': '1.404', 'learning_rate': '3.332e-05', 'epoch': '1.822'}\n",
            "{'loss': '0.1003', 'grad_norm': '0.954', 'learning_rate': '3.212e-05', 'epoch': '1.936'}\n",
            "{'loss': '0.09528', 'grad_norm': '1.482', 'learning_rate': '3.093e-05', 'epoch': '2.05'}\n",
            "{'loss': '0.08631', 'grad_norm': '1.619', 'learning_rate': '2.974e-05', 'epoch': '2.164'}\n",
            "{'loss': '0.07804', 'grad_norm': '1.849', 'learning_rate': '2.854e-05', 'epoch': '2.278'}\n",
            " 46% 1000/2195 [01:26<01:29, 13.31it/s]\n",
            "Writing model shards:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Writing model shards: 100% 1/1 [00:03<00:00,  3.90s/it]\n",
            "{'loss': '0.08834', 'grad_norm': '1.4', 'learning_rate': '2.735e-05', 'epoch': '2.392'}\n",
            "{'loss': '0.08686', 'grad_norm': '1.545', 'learning_rate': '2.616e-05', 'epoch': '2.506'}\n",
            "{'loss': '0.07936', 'grad_norm': '1.17', 'learning_rate': '2.496e-05', 'epoch': '2.62'}\n",
            "{'loss': '0.07791', 'grad_norm': '1.258', 'learning_rate': '2.377e-05', 'epoch': '2.733'}\n",
            "{'loss': '0.08022', 'grad_norm': '1.185', 'learning_rate': '2.258e-05', 'epoch': '2.847'}\n",
            "{'loss': '0.07911', 'grad_norm': '1.999', 'learning_rate': '2.138e-05', 'epoch': '2.961'}\n",
            "{'loss': '0.07704', 'grad_norm': '1.088', 'learning_rate': '2.019e-05', 'epoch': '3.075'}\n",
            "{'loss': '0.07285', 'grad_norm': '1.284', 'learning_rate': '1.9e-05', 'epoch': '3.189'}\n",
            "{'loss': '0.07565', 'grad_norm': '1.626', 'learning_rate': '1.78e-05', 'epoch': '3.303'}\n",
            "{'loss': '0.06931', 'grad_norm': '0.9174', 'learning_rate': '1.661e-05', 'epoch': '3.417'}\n",
            " 68% 1500/2195 [02:12<00:47, 14.79it/s]\n",
            "Writing model shards:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Writing model shards: 100% 1/1 [00:01<00:00,  1.07s/it]\n",
            "{'loss': '0.07662', 'grad_norm': '1.204', 'learning_rate': '1.542e-05', 'epoch': '3.531'}\n",
            "{'loss': '0.06445', 'grad_norm': '1.519', 'learning_rate': '1.422e-05', 'epoch': '3.645'}\n",
            "{'loss': '0.07544', 'grad_norm': '1.024', 'learning_rate': '1.303e-05', 'epoch': '3.759'}\n",
            "{'loss': '0.07656', 'grad_norm': '1.703', 'learning_rate': '1.184e-05', 'epoch': '3.872'}\n",
            "{'loss': '0.06526', 'grad_norm': '1.768', 'learning_rate': '1.064e-05', 'epoch': '3.986'}\n",
            "{'loss': '0.06723', 'grad_norm': '1.316', 'learning_rate': '9.451e-06', 'epoch': '4.1'}\n",
            "{'loss': '0.06415', 'grad_norm': '0.8892', 'learning_rate': '8.258e-06', 'epoch': '4.214'}\n",
            "{'loss': '0.0714', 'grad_norm': '1.124', 'learning_rate': '7.064e-06', 'epoch': '4.328'}\n",
            "{'loss': '0.06999', 'grad_norm': '0.9337', 'learning_rate': '5.871e-06', 'epoch': '4.442'}\n",
            "{'loss': '0.06341', 'grad_norm': '1.136', 'learning_rate': '4.678e-06', 'epoch': '4.556'}\n",
            " 91% 2000/2195 [02:51<00:13, 14.45it/s]\n",
            "Writing model shards:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Writing model shards: 100% 1/1 [00:01<00:00,  1.21s/it]\n",
            "{'loss': '0.07081', 'grad_norm': '1.108', 'learning_rate': '3.484e-06', 'epoch': '4.67'}\n",
            "{'loss': '0.06016', 'grad_norm': '2.036', 'learning_rate': '2.291e-06', 'epoch': '4.784'}\n",
            "{'loss': '0.06157', 'grad_norm': '1.187', 'learning_rate': '1.098e-06', 'epoch': '4.897'}\n",
            "100% 2194/2195 [03:12<00:00, 14.47it/s]\n",
            "Writing model shards:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Writing model shards: 100% 1/1 [00:01<00:00,  1.29s/it]\n",
            "{'train_runtime': '197', 'train_samples_per_second': '22.26', 'train_steps_per_second': '11.14', 'train_loss': '0.1967', 'epoch': '5'}\n",
            "100% 2195/2195 [03:17<00:00, 11.14it/s]\n",
            "Writing model shards: 100% 1/1 [00:01<00:00,  1.41s/it]\n",
            "2026-02-04 09:08:04,293 - src.utils - INFO - Model saved to /content/drive/MyDrive/LLMSQL2/results/gpt2-geography/final\n"
          ]
        }
      ],
      "source": [
        "# Train GPT-2 on selected datasets\n",
        "for db in DATASETS_TO_TRAIN:\n",
        "    data_path = f\"/content/Information-Systems/dthivaios/LLMSQL2/data/text2sql-data/data/{db}.json\"\n",
        "    out_path = f\"/content/drive/MyDrive/LLMSQL2/results/gpt2-{db}\"\n",
        "    print(f\"\\n=== Training GPT-2 on {db} ===\")\n",
        "    !python -m src.train_gpt2 \\\n",
        "        --data \"$data_path\" \\\n",
        "        --output \"$out_path\" \\\n",
        "        --epochs {EPOCHS['gpt2']} --batch-size {BATCH_SIZE_GPT2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c61fafe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c61fafe",
        "outputId": "e1840758-704f-4bb8-b766-b0ff4b1afa98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-04 09:08:22,091 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n",
            "2026-02-04 09:08:23,095 - datasets - INFO - TensorFlow version 2.19.0 available.\n",
            "2026-02-04 09:08:23,096 - datasets - INFO - JAX version 0.7.2 available.\n",
            "2026-02-04 09:08:23,349 - src.utils - INFO - Starting TinyLlama training with config: TrainingConfig(model_name='ManthanKulakarni/TinyLlama-1.1B-Text2SQL', output_dir='/content/drive/MyDrive/LLMSQL2/results/tinyllama-geography', num_epochs=3, batch_size=2, learning_rate=0.0002, max_length=384, warmup_steps=50, save_steps=200, logging_steps=25, use_lora=True, lora_r=16, lora_alpha=32, lora_dropout=0.05)\n",
            "2026-02-04 09:08:23,349 - src.utils - INFO - Database: geography\n",
            "2026-02-04 09:08:23,349 - src.utils - INFO - Loading model: ManthanKulakarni/TinyLlama-1.1B-Text2SQL\n",
            "2026-02-04 09:08:23,524 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/ManthanKulakarni/TinyLlama-1.1B-Text2SQL/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-04 09:08:23,562 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/ManthanKulakarni/TinyLlama-1.1B-Text2SQL/b0198f82cb4f93abac3ca3b5fab5bb72fd012933/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-04 09:08:23,660 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/ManthanKulakarni/TinyLlama-1.1B-Text2SQL/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-04 09:08:23,700 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/ManthanKulakarni/TinyLlama-1.1B-Text2SQL/b0198f82cb4f93abac3ca3b5fab5bb72fd012933/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-04 09:08:23,790 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/ManthanKulakarni/TinyLlama-1.1B-Text2SQL/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-04 09:08:23,802 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/ManthanKulakarni/TinyLlama-1.1B-Text2SQL/b0198f82cb4f93abac3ca3b5fab5bb72fd012933/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-04 09:08:23,891 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/ManthanKulakarni/TinyLlama-1.1B-Text2SQL/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-04 09:08:23,985 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/ManthanKulakarni/TinyLlama-1.1B-Text2SQL/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
            "2026-02-04 09:08:24,227 - src.utils - INFO - Loading model with 8-bit quantization for LoRA training (GPU detected)...\n",
            "2026-02-04 09:08:24,314 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/ManthanKulakarni/TinyLlama-1.1B-Text2SQL/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-04 09:08:24,325 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/ManthanKulakarni/TinyLlama-1.1B-Text2SQL/b0198f82cb4f93abac3ca3b5fab5bb72fd012933/config.json \"HTTP/1.1 200 OK\"\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "2026-02-04 09:08:24,417 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/ManthanKulakarni/TinyLlama-1.1B-Text2SQL/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-04 09:08:24,430 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/ManthanKulakarni/TinyLlama-1.1B-Text2SQL/b0198f82cb4f93abac3ca3b5fab5bb72fd012933/config.json \"HTTP/1.1 200 OK\"\n",
            "Loading weights: 100% 201/201 [00:10<00:00, 19.10it/s, Materializing param=model.norm.weight]\n",
            "2026-02-04 09:08:38,731 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/ManthanKulakarni/TinyLlama-1.1B-Text2SQL/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-04 09:08:38,770 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/ManthanKulakarni/TinyLlama-1.1B-Text2SQL/b0198f82cb4f93abac3ca3b5fab5bb72fd012933/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-04 09:08:38,783 - src.utils - INFO - Applying LoRA configuration...\n",
            "trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n",
            "2026-02-04 09:08:38,922 - src.utils - INFO - Loaded 246 examples from /content/Information-Systems/dthivaios/LLMSQL2/data/text2sql-data/data/geography.json\n",
            "2026-02-04 09:08:38,923 - src.utils - INFO - Prepared 877 training examples\n",
            "Map: 100% 877/877 [00:00<00:00, 1705.01 examples/s]\n",
            "2026-02-04 09:08:39,501 - src.utils - INFO - Starting training...\n",
            "  0% 0/330 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "  2% 5/330 [00:18<18:56,  3.50s/it]"
          ]
        }
      ],
      "source": [
        "# Train TinyLlama (LoRA) on selected datasets\n",
        "for db in DATASETS_TO_TRAIN:\n",
        "    data_path = f\"/content/Information-Systems/dthivaios/LLMSQL2/data/text2sql-data/data/{db}.json\"\n",
        "    out_path = f\"/content/drive/MyDrive/LLMSQL2/results/tinyllama-{db}\"\n",
        "    print(f\"\\n=== Training TinyLlama on {db} ===\")\n",
        "    !python -m src.train_tinyllama \\\n",
        "        --data \"$data_path\" \\\n",
        "        --output \"$out_path\" \\\n",
        "        --epochs {EPOCHS['tinyllama']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0e840f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0e840f8",
        "outputId": "ab31111d-02b9-4bee-f89e-d9c93727b81a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-04 08:56:11,207 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n",
            "<frozen runpy>:128: RuntimeWarning: 'src.evaluation' found in sys.modules after import of package 'src', but prior to execution of 'src.evaluation'; this may result in unpredictable behaviour\n"
          ]
        }
      ],
      "source": [
        "# Evaluate GPT-2 fine-tuned models\n",
        "for db in DATASETS_TO_TRAIN:\n",
        "    checkpoint = f\"/content/drive/MyDrive/LLMSQL2/results/gpt2-{db}/final\"\n",
        "    print(f\"\\n=== Evaluating GPT-2 on {db} ===\")\n",
        "    !python -m src.evaluation \\\n",
        "        --model gpt2 \\\n",
        "        --checkpoint \"$checkpoint\" \\\n",
        "        --database \"$db\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7402bef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7402bef",
        "outputId": "89ebfc23-6c49-4e58-e4c6-58c6b7a6a8cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-04 08:56:19,897 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n",
            "<frozen runpy>:128: RuntimeWarning: 'src.evaluation' found in sys.modules after import of package 'src', but prior to execution of 'src.evaluation'; this may result in unpredictable behaviour\n"
          ]
        }
      ],
      "source": [
        "# Evaluate TinyLlama fine-tuned models\n",
        "for db in DATASETS_TO_TRAIN:\n",
        "    checkpoint = f\"/content/drive/MyDrive/LLMSQL2/results/tinyllama-{db}/final\"\n",
        "    print(f\"\\n=== Evaluating TinyLlama on {db} ===\")\n",
        "    !python -m src.evaluation \\\n",
        "        --model tinyllama \\\n",
        "        --checkpoint \"$checkpoint\" \\\n",
        "        --database \"$db\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13812d25",
      "metadata": {
        "id": "13812d25"
      },
      "source": [
        "## Next Steps\n",
        "- Update `DATASETS_TO_TRAIN` to include any remaining databases.\n",
        "- If you want to re-train geography or atis, just add them to the list.\n",
        "\n",
        "Example data paths (auto-generated by the loop):\n",
        "- /content/Information-Systems/dthivaios/LLMSQL2/data/text2sql-data/data/advising.json\n",
        "- /content/Information-Systems/dthivaios/LLMSQL2/data/text2sql-data/data/atis.json\n",
        "- /content/Information-Systems/dthivaios/LLMSQL2/data/text2sql-data/data/restaurants.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
